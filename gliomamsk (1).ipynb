import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, max_error
import shap

# Assuming you have a CSV file as your dataset
data = pd.read_csv()#your file
#datacoadmskML4
# Handle missing values (if any)
data = data.dropna()

# Encode categorical variables (assuming they are object types)
data_encoded = pd.get_dummies(data, drop_first=True)

# Split the data into features (X) and target variable (y)
X = data_encoded.drop('Overall_Survival_Months', axis=1)  # Features
y = data_encoded['Overall_Survival_Months']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Hyperparameter Tuning using RandomizedSearchCV

# Random Forest
param_dist_rf = {
    'n_estimators': [10, 20, 50, 100, 150, 200, 250, 300, 350, 500, 750, 1000],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10, 15, 20, 50, 100],
    'min_samples_leaf': [1, 2, 4, 8, 12, 15, 20]
}
rf_model = RandomForestRegressor(random_state=42)
random_search_rf = RandomizedSearchCV(rf_model, param_distributions=param_dist_rf, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
random_search_rf.fit(X_train_scaled, y_train)
best_rf_model = random_search_rf.best_estimator_

# Gradient Boosting
param_dist_gb = {
    'n_estimators': [10, 20, 50, 100, 150, 200, 250, 300, 350, 500, 750, 1000],
    'max_depth': [3, 5, 7, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10, 15, 20, 50, 100],
    'min_samples_leaf': [1, 2, 4, 8, 12, 15, 20],
    'learning_rate': [0.01, 0.1, 0.2, 0.5]
}
gb_model = GradientBoostingRegressor(random_state=42)
random_search_gb = RandomizedSearchCV(gb_model, param_distributions=param_dist_gb, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
random_search_gb.fit(X_train_scaled, y_train)
best_gb_model = random_search_gb.best_estimator_

# MLP Regression
mlp_model = MLPRegressor(random_state=42)
mlp_model.fit(X_train_scaled, y_train)

#other models
LR_model = LinearRegression()
LR_model.fit(X_train_scaled, y_train)

Lasso_model = Lasso()
Lasso_model.fit(X_train_scaled, y_train)

SVR_model = SVR()
SVR_model.fit(X_train_scaled, y_train)

KNR_model = KNeighborsRegressor()
KNR_model.fit(X_train_scaled, y_train)

DTR_model = DecisionTreeRegressor()
DTR_model.fit(X_train_scaled, y_train)

# Define models and model_names
models = [best_rf_model, best_gb_model, mlp_model, LR_model, Lasso_model, SVR_model, KNR_model, DTR_model]
model_names = ['Random Forest', 'Gradient Boosting', 'MLP', 'LinearRegression', 'Lasso', 'SVR', 'KNeighborsRegressor', 'DecisionTreeRegressor']

# Model Evaluation with Cross-Validation
for model, name in zip(models, model_names):
    predictions = model.predict(X_test_scaled)

    mse = mean_squared_error(y_test, predictions)
    mae = mean_absolute_error(y_test, predictions)
    medae = median_absolute_error(y_test, predictions)
    maxe = max_error(y_test, predictions)

    print(f"\n{name} Metrics:")
    print("Mean Squared Error:", mse)
    print("Mean Absolute Error:", mae)
    print("Median Absolute Error:", medae)
    print("Max Error:", maxe)
    
    # Perform cross-validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
    
    # Print cross-validation results
    print(f"\n{name} Cross-Validation Scores:")
    print("Mean MSE:", -cv_scores.mean())
    print("Std MSE:", cv_scores.std())
    
    # Scatter Plot
    plt.figure(figsize=(8, 8))
    plt.scatter(y_test, predictions)
    plt.title(f'{name} - Predicted vs Actual')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.xlim([0, max(y_test)+1])
    plt.ylim([0, max(y_test)+1])
    plt.gca().set_aspect('equal', adjustable='box')  # Set aspect ratio to be equal
    plt.show()
    
# Example for using Shapley values with a model
explainerrf = shap.Explainer(best_rf_model)
shap_valuesrf = explainerrf.shap_values(X_test_scaled)
shap.summary_plot(shap_valuesrf, X_test_scaled, feature_names=X.columns)

explainergb = shap.Explainer(best_gb_model)
shap_valuesgb = explainergb.shap_values(X_test_scaled)
shap.summary_plot(shap_valuesgb, X_test_scaled, feature_names=X.columns)
